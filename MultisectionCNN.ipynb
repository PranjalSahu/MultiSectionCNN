{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ALL THE IMPORTS\n",
    "\n",
    "import keras\n",
    "from __future__ import print_function\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Maximum, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, MaxPooling1D, Reshape, GaussianNoise, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from random import randint\n",
    "from scipy.ndimage import rotate\n",
    "import math\n",
    "from scipy import misc\n",
    "import scipy.ndimage\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import regularizers\n",
    "from numpy.random import permutation\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import sys, glob, argparse\n",
    "import numpy as np\n",
    "import math, cv2\n",
    "from scipy.stats import multivariate_normal\n",
    "import time\n",
    "from sklearn import svm\n",
    "from numpy.testing import assert_allclose\n",
    "#from nose.tools import raises\n",
    "#from cyvlfeat.gmm import gmm\n",
    "#from cyvlfeat.fisher import fisher\n",
    "from skimage.measure import shannon_entropy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pylidc as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import find_contours\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import RemoteMonitor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from scipy.misc import imresize\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.misc import imread\n",
    "from keras import backend as k\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imsave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO GET THE BASE MODEL\n",
    "\n",
    "def get_mobilenet_model():\n",
    "    base_model  = MobileNet(input_shape=(224,224,3), include_top=False)\n",
    "    #base_model  = InceptionV3(input_shape=(224,224,3), include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    sgd = optimizers.SGD(lr=0.0001)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    #print (model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files(files, test_data, test_data_label):\n",
    "    for f in files:\n",
    "        t = imread(f)\n",
    "        t = t/255.0\n",
    "        test_data.append(t)\n",
    "        fn = '_'.join(f.split('/')[-1].split('.')[0].split('_')[1:])\n",
    "        #fn = '_'.join(f.split('/')[-1].split('.')[0].split('_'))\n",
    "        test_data_label.append(fn)\n",
    "    return [test_data, test_data_label]\n",
    "\n",
    "# Function to read test data\n",
    "def get_test_data(crsd):\n",
    "    files1 = glob.glob(filepath_a+'fold'+str(crsd)+'/Test/1/*.jpg')\n",
    "    files2 = glob.glob(filepath_a+'fold'+str(crsd)+'/Test/2/*.jpg')\n",
    "    test_data = []\n",
    "    test_data_label = []\n",
    "    #print(files1)\n",
    "    test_data, test_data_label = read_files(files1, test_data, test_data_label)\n",
    "    test_data, test_data_label = read_files(files2, test_data, test_data_label)\n",
    "    \n",
    "    #print('test data label ', test_data_label)\n",
    "    \n",
    "    malignancy_label_test = []\n",
    "    file_label    = []\n",
    "    ml            = []\n",
    "    orig_filename = []\n",
    "    \n",
    "    for idk, t in enumerate(test_data_label):\n",
    "        label_data = t.split('_')\n",
    "        pv = t.split('_')[-2]\n",
    "        temp = [0, 0]\n",
    "        temp[int(pv)-1] = 1\n",
    "        malignancy_label_test.append(temp)\n",
    "        file_label.append(t)\n",
    "        ml.append(pv)\n",
    "    \n",
    "    malignancy_label_test = np.array(malignancy_label_test)\n",
    "    test_data             = np.array(test_data)\n",
    "    \n",
    "    return [test_data, malignancy_label_test, file_label, ml]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING FOR CLASSIFICATION\n",
    "\n",
    "# Folder structure\n",
    "# image_sections\n",
    "#       -- fold1\n",
    "#           -- Train\n",
    "#              -- 1\n",
    "#              -- 2\n",
    "#           -- Validation\n",
    "#              -- 1\n",
    "#              -- 2\n",
    "#           -- Test\n",
    "#              -- 1\n",
    "#              -- 2\n",
    "\n",
    "\n",
    "ensemble     = 1\n",
    "init_weights =  None\n",
    "\n",
    "filesc     = glob.glob('/media/pranjal/part2/FILTERDATANEW_h/*.npy')\n",
    "filepath_a = '/image_sections/'\n",
    "batch_size = 32\n",
    "crsd       = 1\n",
    "\n",
    "print('FOLD STARTED ', crsd)\n",
    "\n",
    "# Obtain the base model\n",
    "model = get_mobilenet_model()\n",
    "\n",
    "# Create the Data generators for Train and validation splits\n",
    "train_datagen      = ImageDataGenerator(rescale=1./255, rotation_range=180.0,\n",
    "                                        horizontal_flip=True, vertical_flip=True)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255, rotation_range=180.0,\n",
    "                                        horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "   filepath_a+'/fold'+str(crsd)+'/Train/',\n",
    "   batch_size=32,\n",
    "   target_size=(224, 224))\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "   filepath_a+'/fold'+str(crsd)+'/Validation/',\n",
    "   batch_size=128,\n",
    "   target_size=(224, 224))\n",
    "\n",
    "filepath   ='/modelspath/'+str(crsd)+'_mobilenet_weights.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "reduce_lr  = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0000001)\n",
    "er         = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model using fit_generator\n",
    "model.fit_generator(train_generator,\n",
    "  steps_per_epoch=400,\n",
    "  epochs=50,\n",
    "  validation_data=validation_generator,\n",
    "  validation_steps=40,\n",
    "  callbacks=[reduce_lr, checkpoint, er])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For predicting using only one image at a time using the above trained model\n",
    "\n",
    "# Restore the weight with best validation loss\n",
    "model.load_weights(filepath)\n",
    "\n",
    "train_data      = []\n",
    "validation_data = []\n",
    "\n",
    "test_data, malignancy_label_test, file_label, ml = get_test_data(crsd)\n",
    "\n",
    "predictions           = model.predict(test_data, batch_size=100)\n",
    "all_predictions_value = predictions\n",
    "predictions_prob      = predictions\n",
    "\n",
    "ruc_score = roc_auc_score(malignancy_label_test, predictions)\n",
    "cm    = confusion_matrix(np.argmax(predictions, axis=1), np.argmax(malignancy_label_test, axis=1))\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "\n",
    "print(\"RUC :\", ruc_score, \"SENSITIVITY :\", sensitivity, \"SPECIFICITY :\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_name(name):\n",
    "    fn = '_'.join(name.split('/')[-1].split('.')[0].split('_')[1:])\n",
    "    return fn\n",
    "#print(get_file_name(files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FOR TAKING MAX POOLING OF FEATURES OF ALL IMAGE SECTIONS FOR ONE NODULE\n",
    "\n",
    "crsd  = 1\n",
    "model = get_mobilenet_model()\n",
    "split = 'Validation'\n",
    "b     = 32\n",
    "ensemble = 1\n",
    "\n",
    "def get_data_from_folder(files, label, start, batch):\n",
    "    test_data = []\n",
    "    test_data_label = []\n",
    "    for j in range(start, start+batch):\n",
    "    #for j in range((batch)):\n",
    "        test_data.append(imread(files[j])/255.0)\n",
    "        #fn = files[j]\n",
    "        fn = get_file_name(files[j])\n",
    "        test_data_label.append(fn)\n",
    "    test_data = np.array(test_data)\n",
    "    test_data_label = np.array(test_data_label)\n",
    "    return test_data, test_data_label\n",
    "\n",
    "def get_features(files1, files2):\n",
    "    test_data_label = []\n",
    "    features        = []\n",
    "    \n",
    "    #for i in range(len(files1)):\n",
    "    for i in range(int(len(files1)/128)):\n",
    "        start = i*128\n",
    "        test_data_temp, test_data_label_temp = get_data_from_folder(files1, [1, 0], start, 128)\n",
    "        #print('test_data_temp shape is ', test_data_temp.shape)\n",
    "        features_temp = intermediate_layer_model.predict(test_data_temp, batch_size=128)\n",
    "        #print('features_temp 1 ', start)\n",
    "        for t in features_temp:\n",
    "            features.append(t)\n",
    "        for t in test_data_label_temp:\n",
    "            test_data_label.append(t)\n",
    "    \n",
    "    #for i in range(len(files2)):\n",
    "    for i in range(int(len(files2)/128)):\n",
    "        start = i*128\n",
    "        test_data_temp, test_data_label_temp = get_data_from_folder(files2, [0, 1], start, 128)\n",
    "        features_temp = intermediate_layer_model.predict(test_data_temp, batch_size=128)\n",
    "        #print('features_temp 2 ', start)\n",
    "        for t in features_temp:\n",
    "            features.append(t)\n",
    "        for t in test_data_label_temp:\n",
    "            test_data_label.append(t)\n",
    "    #test_data_label = np.array(test_data_label)\n",
    "    #features        = np.array(features)\n",
    "    #print(len(features), len(test_data_label.shape))\n",
    "    return features, test_data_label\n",
    "\n",
    "crsd = 1\n",
    "model.load_weights('/modelspath/'str(crsd)+'_mobilenet_weights.h5')\n",
    "\n",
    "files1 = glob.glob('/imagesections/fold'+str(crsd)+'/Validation/1/*.jpg')\n",
    "files2 = glob.glob('/imagesections/fold'+str(crsd)+'/Validation/2/*.jpg')\n",
    "\n",
    "# Get the output of fully connected layer\n",
    "layer_name = model.layers[-2].name\n",
    "intermediate_layer_model  = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "features, test_data_label = get_features(files1, files2)\n",
    "\n",
    "h = {}\n",
    "for i, t in enumerate(test_data_label):\n",
    "    h.setdefault(t, [])\n",
    "    h[t].append(features[i])\n",
    "for k in h:\n",
    "    h[k] = np.max(h[k], axis=0)\n",
    "features_name = []\n",
    "features = []\n",
    "for k in h:\n",
    "    features_name.append(k)\n",
    "    features.append(h[k])\n",
    "features      = np.array(features)\n",
    "features_name = np.array(features_name)\n",
    "\n",
    "print('SAVING FEATURES FILE ', crsd)\n",
    "print(features_name.shape)\n",
    "print(features.shape)\n",
    "\n",
    "np.save('/featurespath/fold-'+str(crsd)+'/'+split+'_mobilenet_features.h5',       features)\n",
    "np.save('/featurespath/fold-'+str(crsd)+'/'+split+'_mobilenet_features_names.h5', features_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TAKES THE ORIGINAL MODEL AS INPUT AND OUTPUTS THE LATER HALF OF THE MODEL\n",
    "\n",
    "def get_model_pool(model):\n",
    "    main_input = Input(shape=(1024, ))\n",
    "    malignancy        = Dense(2, activation='softmax')(main_input)\n",
    "    pool_model = Model(inputs=[main_input], outputs=[malignancy])\n",
    "    # SETTING WEIGHTS FOR LATER HALF\n",
    "    layer_name = model.layers[-1].name\n",
    "    layer      = model.get_layer(layer_name)\n",
    "    weights    = layer.get_weights()\n",
    "    #sgd = optimizers.SGD(lr=0.0001)\n",
    "    pool_model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam',\n",
    "                      metrics=[metrics.categorical_accuracy])\n",
    "    new_layer_name = pool_model.layers[-1].name\n",
    "    newlayer       = pool_model.get_layer(new_layer_name)\n",
    "    newlayer.set_weights(weights)\n",
    "    return pool_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FOR PREDICTION USING MAX POOLING OF FEATURES FOR STAGE 2 TRAINING AND PREDICTION\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "crsd  = 1\n",
    "model = get_mobilenet_model()\n",
    "\n",
    "split = 'Test'\n",
    "b     = 32\n",
    "ensemble  = 1\n",
    "all_accu  = []\n",
    "all_accua = []\n",
    "all_mse = []\n",
    "\n",
    "values = {}\n",
    "files  = glob.glob('/media/pranjal/de24af8d-2361-4ea2-a07a-1801b54488d9/FILTERDATANEW_h/*')\n",
    "for f in files:\n",
    "    tp = f.split('/')[-1]\n",
    "    a  = tp.split('.')[0]\n",
    "    b  = float(tp.split('_')[-1][0:3])\n",
    "    values[a] = b\n",
    "\n",
    "crsd = 1\n",
    "traindata       = np.load('/featurespath/fold-'+str(crsd)+'/'+split+'_mobilenet_features.h5.npy') \n",
    "nodulenametrain = np.load('/featurespath/fold-'+str(crsd)+'/'+split+'_mobilenet_features_names.h5.npy')\n",
    "\n",
    "maxpool    = np.load('/featurespath/fold-'+str(crsd)+'_'+split+'_mobilenet_features.h5.npy')\n",
    "nodulename = np.load('/featurespath/fold-'+str(crsd)+'_'+split+'_mobilenet_features_names.h5.npy')\n",
    "\n",
    "h           = {}\n",
    "final_label = []\n",
    "train_label = []\n",
    "\n",
    "for t in nodulename:\n",
    "    final_label.append(values[t])\n",
    "for t in nodulenametrain:\n",
    "    train_label.append(values[t])\n",
    "\n",
    "train_label = np.array(train_label)\n",
    "final_label = np.array(final_label)\n",
    "\n",
    "p = np.random.permutation(len(traindata))\n",
    "traindata   = traindata[p]\n",
    "train_label = train_label[p]\n",
    "\n",
    "#clf = LogisticRegression(solver='liblinear',n_jobs=-1, random_state=0, \n",
    "#                    max_iter=155,  class_weight='balanced', intercept_scaling=0.01, C=0.01, tol=0.005)\n",
    "#clf = SVR(C=1.0, epsilon=0.2, tol=0.0001, gamma=0.0001)\n",
    "#clf = RandomForestRegressor(max_depth=15, random_state=0)\n",
    "#clf = SGDRegressor(tol=0.001)\n",
    "\n",
    "#clf.fit(traindata, train_label)\n",
    "#clf.fit(traindata, train_label.astype(int))\n",
    "\n",
    "model.fit([traindata], [train_label], epochs=1000, batch_size=128, \n",
    "          shuffle=True, verbose=True, validation_split=0.1, callbacks=[reduce_lr, er, checkpoint])\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(maxpool)\n",
    "\n",
    "count     = 0\n",
    "total     = len(predictions)\n",
    "incorrect = 0\n",
    "#y_test    = final_label.astype(int)\n",
    "y_test    = final_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CODE TO GENERATE TRAINING DATA BY TAKING MULTIPLE CROSS-SECTIONS\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from   sklearn.model_selection import KFold\n",
    "\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import misc\n",
    "import glob\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "count      = 0\n",
    "checkcount = 0\n",
    "checked = {}\n",
    "checked_a = {}\n",
    "checked_b = {}\n",
    "\n",
    "files   = glob.glob('/volumes/volume*.npy') \n",
    "filessc = []\n",
    "for f in files:\n",
    "    at = f.replace('volume-', '')\n",
    "    filessc.append(at)\n",
    "files = filessc\n",
    "\n",
    "alldata = {}\n",
    "\n",
    "for name in files:\n",
    "\tnodule_id = name.split('.')[0]\n",
    "\tchecked[nodule_id]   = name\n",
    "\tchecked_a[nodule_id] = name.split('.')[0].split('/')[-1]\n",
    "\tchecked_b[nodule_id] = name.split('_')[3]\n",
    "\talldata[nodule_id]   = []\n",
    "\n",
    "def rotation_matrix(axis, theta):\n",
    "    axis = np.asarray(axis)\n",
    "    axis = axis/math.sqrt(np.dot(axis, axis))\n",
    "    a = math.cos(theta/2.0)\n",
    "    b, c, d = -axis*math.sin(theta/2.0)\n",
    "    aa, bb, cc, dd = a*a, b*b, c*c, d*d\n",
    "    bc, ad, ac, ab, bd, cd = b*c, a*d, a*c, a*b, b*d, c*d\n",
    "    return np.array([[aa+bb-cc-dd, 2*(bc+ad), 2*(bd-ac)],\n",
    "                     [2*(bc-ad), aa+cc-bb-dd, 2*(cd+ab)],\n",
    "                     [2*(bd+ac), 2*(cd-ab), aa+dd-bb-cc]])\n",
    "from skimage import filters\n",
    "\n",
    "def get_empty_classes():\n",
    "    classes = {}\n",
    "    classes['1']   = []\n",
    "    classes['2']   = []\n",
    "    return classes\n",
    "\n",
    "\n",
    "keys = list(checked.keys())\n",
    "random.seed(450)\n",
    "random.shuffle(keys)\n",
    "\n",
    "image_size = 50\n",
    "\n",
    "point = np.array([image_size/2, image_size/2, image_size/2])\n",
    "temp  = np.zeros((image_size, image_size))\n",
    "count = 0\n",
    "\n",
    "divisions = 12\n",
    "div       = math.pi/divisions\n",
    "for name in checked:\n",
    "    a     = np.load(checked[name])\n",
    "    atx = '/volumes/volume-'+checked[name][len('/volumes/'):]\n",
    "    a_seg = np.load(atx)\n",
    "    namep = checked_a[name]\n",
    "    count = 0\n",
    "    mode_malignancy = namep.split('_')[-2]\n",
    "    if mode_malignancy == '2':\n",
    "        divisions = 7\n",
    "    else:\n",
    "        divisions = 5\n",
    "    div = math.pi/divisions\n",
    "    print(name)\n",
    "    \n",
    "    angles_array = [[0, 0], [math.pi/2, 0], [math.pi/2, math.pi/2]]\n",
    "    #angles_array = [[0, 0] ]\n",
    "    \n",
    "    #for divb in range(0, divisions + 1):\n",
    "    for abc in range(1):\n",
    "        for abc_i, abc_j in enumerate(angles_array):\n",
    "        #for diva in range(0, divisions + 1):\n",
    "            #anglea = diva * div\n",
    "            #angleb = divb * div\n",
    "            anglea = angles_array[abc_i][0]\n",
    "            angleb = angles_array[abc_i][1]\n",
    "            \n",
    "            x = math.cos(anglea) * math.sin(angleb)\n",
    "            y = math.sin(anglea) * math.sin(angleb)\n",
    "            z = math.cos(angleb)\n",
    "            normal = np.array([x, y, z])\n",
    "            for (i, val) in enumerate(normal):\n",
    "                if abs(normal[i]) < 0.000001:\n",
    "                    normal[i] = 0\n",
    "            (yy, zz) = np.meshgrid(range(image_size), range(image_size))\n",
    "            yy = yy - image_size/2\n",
    "            zz = zz - image_size/2\n",
    "            xx = temp\n",
    "            rt1 = rotation_matrix([0, 0, 1], anglea)\n",
    "            a1 = np.reshape(xx, -1)\n",
    "            a2 = np.reshape(yy, -1)\n",
    "            a3 = np.reshape(zz, -1)\n",
    "            val1 = np.array([a1, a2, a3])\n",
    "            val1_prime = np.dot(rt1, val1)\n",
    "            new_axis   = np.dot([0, 1, 0], rt1)\n",
    "            mag = np.sqrt(new_axis.dot(new_axis))\n",
    "            new_axis = new_axis / mag\n",
    "            rt2 = rotation_matrix(new_axis, angleb)\n",
    "            val2_prime = np.dot(rt2, val1_prime)\n",
    "            val2_prime = val2_prime + image_size/2\n",
    "            sp = scipy.ndimage.map_coordinates(a, val2_prime, order=3, mode='nearest')\n",
    "            sp = np.reshape(sp, [image_size, image_size])\n",
    "            #sp = imresize(sp, (224, 224))\n",
    "            #sp = sp.astype(float)\n",
    "            #sp = sp/255.0\n",
    "            temp_data = [namep, sp]\n",
    "            alldata[name].append(temp_data)\n",
    "            count = count + 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
